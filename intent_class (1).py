# -*- coding: utf-8 -*-
"""intent_class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBsFNDu9VYU3e3FNZzTYmvVqbWdX4g7y
"""

!pip install -q pandas torch scikit-learn

"""Setup and Imports"""

import time
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import re
import pickle
import csv

print("Libraries imported successfully!")

"""Load and Preprocess Data"""

# --- Adapted Data Loading ---
# Load your dataset from the 'intent.csv' file.
# The new format is a standard CSV with a header, so we can read it directly.
try:
    df = pd.read_csv('intent.csv')
    # The new CSV has columns 'text' and 'label'.
    # We rename 'label' to 'intent' to match the rest of the code.
    df.rename(columns={'label': 'intent'}, inplace=True)
    print("Successfully loaded and adapted 'intent.csv'.")

except FileNotFoundError:
    print("Error: 'intent.csv' not found.")
    print("Please make sure you have uploaded the file.")
    # This dummy dataframe is kept as a fallback.
    data = {'text': ['tell me a joke', 'what is the weather like', 'play some music',
                     'how are you', 'what is the temperature', 'I want to hear a song'],
            'intent': ['joke', 'weather', 'music', 'greeting', 'weather', 'music']}
    df = pd.DataFrame(data)
    print("\nUsing a dummy dataframe for demonstration.")
except Exception as e:
    print(f"An error occurred while reading the file: {e}")


# --- Inspect the class distribution ---
# This part remains the same and now works with the adapted dataframe.
print("\n--- Examining Intent Counts ---")
print("This shows how many examples exist for each intent category:")
print(df['intent'].value_counts())
print("---------------------------------")


# Basic text cleaning function
def clean_text(text):
    if not isinstance(text, str):
        return ""
    # Substitute non-alphabetic characters (and not whitespace) with nothing
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A)
    text = text.lower()
    text = text.strip()
    return text

df['text'] = df['text'].apply(clean_text)

# Encode the labels
label_encoder = LabelEncoder()
df['intent_encoded'] = label_encoder.fit_transform(df['intent'])
print("\nLabels encoded:")
# Creates a dictionary to easily see the mapping from text label to numeric code
print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))


# Split data into training and validation sets
# The user's original comment about 'stratify' is preserved.
# --- CORRECTED LINE: Removed 'stratify' parameter ---
X_train, X_val, y_train, y_val = train_test_split(
    df['text'],
    df['intent_encoded'],
    test_size=0.2,
    random_state=42
)

print(f"\nTraining set size: {len(X_train)}")
print(f"Validation set size: {len(X_val)}")

print("\nFinal processed DataFrame head:")
print(df.head())

"""Build Vocabulary"""

# Build vocabulary from the training data
counter = Counter()
for text in X_train:
    counter.update(text.split())

# Create a word-to-index dictionary
# Start with special tokens
word_to_idx = {'<pad>': 0, '<unk>': 1}
# Start indexing from 2, since 0 and 1 are taken
idx = 2
for word, count in counter.most_common():
    word_to_idx[word] = idx
    idx += 1

vocab_size = len(word_to_idx)
print(f"Vocabulary size: {vocab_size}")

# Example: Get the index of a word
print(f"Index of 'the': {word_to_idx.get('the', 1)}") # Use .get() to handle unknown words

"""Create PyTorch Dataset"""

class IntentDataset(Dataset):
    def __init__(self, texts, labels, vocabulary, max_len=20):
        self.texts = texts
        self.labels = labels
        self.vocabulary = vocabulary
        self.max_len = max_len
        self.unk_token_idx = vocabulary['<unk>']
        self.pad_token_idx = vocabulary['<pad>']

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = self.labels.iloc[idx]

        # Tokenize and numericalize using the dictionary
        tokens = text.split()
        # Use .get() to default to the <unk> token index if a word is not in the vocabulary
        token_indices = [self.vocabulary.get(token, self.unk_token_idx) for token in tokens]

        # Pad or truncate the sequence
        if len(token_indices) < self.max_len:
            # Pad with the index of <pad> token
            token_indices.extend([self.pad_token_idx] * (self.max_len - len(token_indices)))
        else:
            token_indices = token_indices[:self.max_len]

        return torch.tensor(token_indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)

# Define max sequence length
MAX_LEN = 25

# Create dataset instances
train_dataset = IntentDataset(X_train, y_train, word_to_idx, MAX_LEN)
val_dataset = IntentDataset(X_val, y_val, word_to_idx, MAX_LEN)

# Example of one item from the dataset
print("Example item from the dataset:")
print(train_dataset[0])

"""Define the Lightweight Model"""

class IntentClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        super(IntentClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        # Using a simple averaging of embeddings instead of a complex RNN/LSTM
        # This is a form of a "Continuous Bag-of-Words" (CBOW) model.
        self.fc = nn.Linear(embed_dim, num_class)

    def forward(self, text):
        # text shape: (batch_size, seq_len)
        embedded = self.embedding(text)
        # embedded shape: (batch_size, seq_len, embed_dim)

        # Average the embeddings across the sequence length dimension
        # The mean operation needs to ignore padding. We can do this by creating a mask.
        mask = (text != 0).float().unsqueeze(2)
        embedded = embedded * mask
        summed = torch.sum(embedded, 1)
        non_pad_count = mask.sum(1)
        mean_embedded = summed / (non_pad_count + 1e-9) # Add epsilon to avoid division by zero
        # mean_embedded shape: (batch_size, embed_dim)

        return self.fc(mean_embedded)

# Hyperparameters
EMBED_DIM = 32 # Keep this small for a lightweight model
NUM_CLASSES = len(label_encoder.classes_)
BATCH_SIZE = 8

# Instantiate the model
model = IntentClassifier(vocab_size, EMBED_DIM, NUM_CLASSES)
print("Model architecture:")
print(model)

# Dataloaders
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

"""Train the Model"""

# Training parameters
LEARNING_RATE = 0.005
EPOCHS = 50 # Increase if needed, but watch for overfitting

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Training loop
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    total_acc = 0

    for texts, labels in train_dataloader:
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total_acc += (predicted == labels).sum().item()

    # Print training stats
    avg_loss = total_loss / len(train_dataloader)
    avg_acc = total_acc / len(train_dataset)
    print(f'Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}, Train Acc: {avg_acc:.4f}')

    # Validation loop
    model.eval()
    val_loss = 0
    val_acc = 0
    with torch.no_grad():
        for texts, labels in val_dataloader:
            outputs = model(texts)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_acc += (predicted == labels).sum().item()

    avg_val_loss = val_loss / len(val_dataloader)
    avg_val_acc = val_acc / len(val_dataset)
    print(f'Epoch {epoch+1}/{EPOCHS}, Val Loss: {avg_val_loss:.4f},   Val Acc: {avg_val_acc:.4f}\n')

print("Training complete!")

"""Save the Model for Mobile"""

# Ensure the model is in evaluation mode
model.eval()

# Create an example input tensor. This is needed for tracing.
# The shape should match a single input to the model.
example_input = torch.randint(0, vocab_size, (1, MAX_LEN), dtype=torch.long)

# Trace the model
traced_script_module = torch.jit.trace(model, example_input)

# Save the traced model
# This .ptl file is the one you will use in your Android app with PyTorch Mobile.
model_filename_mobile = "intent_model_mobile.ptl"
traced_script_module._save_for_lite_interpreter(model_filename_mobile)

print(f"Model saved for mobile deployment as '{model_filename_mobile}'")

# You can also save the regular state dict for later use in Python
model_filename_pytorch = "intent_model_pytorch.pth"
torch.save(model.state_dict(), model_filename_pytorch)
print(f"Standard PyTorch model state_dict saved as '{model_filename_pytorch}'")

# Also save the vocabulary and label encoder, you'll need them for inference
import pickle

with open('vocabulary.pkl', 'wb') as f:
    pickle.dump(word_to_idx, f)

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

print("Vocabulary and Label Encoder also saved.")

"""Evaluation, Inference, and Model Stats"""

# --- 1. Load the necessary artifacts ---
# You would typically do this in your application
# For this notebook, we can reuse the objects from previous cells,
# but we'll load them to demonstrate the full process.

# Load the vocabulary and label encoder
with open('vocabulary.pkl', 'rb') as f:
    word_to_idx = pickle.load(f)

with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

# Re-instantiate the model structure
# Make sure the parameters match the ones used for training
vocab_size = len(word_to_idx)
EMBED_DIM = 32 # Should be the same as in Cell 5
NUM_CLASSES = len(label_encoder.classes_) # Should be the same as in Cell 5
model_eval = IntentClassifier(vocab_size, EMBED_DIM, NUM_CLASSES)

# Load the trained weights
# For evaluation in Python, it's easier to use the .pth file
model_eval.load_state_dict(torch.load("intent_model_pytorch.pth"))
model_eval.eval() # Set the model to evaluation mode

print("Model, vocabulary, and label encoder loaded successfully.\n")


# --- 2. Model Information ---
# Calculate the total number of parameters
total_params = sum(p.numel() for p in model_eval.parameters() if p.requires_grad)
print(f"Total trainable parameters: {total_params:,}")

# Get the size of the saved mobile-optimized model file
try:
    model_size_bytes = os.path.getsize("intent_model_mobile.ptl")
    model_size_kb = model_size_bytes / 1024
    print(f"Size of the mobile model file ('intent_model_mobile.ptl'): {model_size_kb:.2f} KB")
except FileNotFoundError:
    print("Mobile model file not found.")

print("-" * 30)

# --- 3. Prediction Function and Inference Time ---
MAX_LEN = 25 # Should be the same as in Cell 4
unk_token_idx = word_to_idx['<unk>']
pad_token_idx = word_to_idx['<pad>']

def predict_intent(sentence):
    # Preprocess the sentence
    cleaned_sentence = clean_text(sentence)
    tokens = cleaned_sentence.split()

    # Numericalize
    token_indices = [word_to_idx.get(token, unk_token_idx) for token in tokens]

    # Pad/Truncate
    if len(token_indices) < MAX_LEN:
        token_indices.extend([pad_token_idx] * (MAX_LEN - len(token_indices)))
    else:
        token_indices = token_indices[:MAX_LEN]

    # Convert to tensor
    text_tensor = torch.tensor(token_indices, dtype=torch.long).unsqueeze(0) # Add batch dimension

    # Get prediction
    with torch.no_grad():
        output = model_eval(text_tensor)
        _, predicted_idx = torch.max(output.data, 1)

    # Decode the prediction
    predicted_intent = label_encoder.inverse_transform(predicted_idx.numpy())[0]
    return predicted_intent

# --- 4. Evaluate on Test Sentences ---
test_sentences = [
    "How big is this tomato sauce?",
    "Sorry repeat that?",
    "Where can I buy meat?",
    "Show me nutriments infos of this cereal box.",
    "How many gallons is this milk?"
]

print("\nRunning predictions on test sentences:")
for sentence in test_sentences:
    prediction = predict_intent(sentence)
    print(f"Sentence: '{sentence}'")
    print(f"--> Predicted Intent: '{prediction}'\n")

# --- 5. Measure Inference Time ---
# Let's measure the time for one prediction.
# We run it once to warm up, then time it.
_ = predict_intent("this is a warm-up sentence")

start_time = time.perf_counter()
_ = predict_intent("what is the weather like today")
end_time = time.perf_counter()

inference_time_ms = (end_time - start_time) * 1000
print("-" * 30)
print(f"Single sentence inference time: {inference_time_ms:.4f} ms")

import pickle

print("Starting conversion of .pkl files to .txt...")

try:
    # 1. Convert vocabulary.pkl -> vocab.txt
    print("Loading vocabulary.pkl...")
    with open('vocabulary.pkl', 'rb') as f:
        vocabulary = pickle.load(f)

    print("Saving as vocab.txt...")
    with open('vocab.txt', 'w', encoding='utf-8') as f:
        # Assumes vocabulary is a dict like {'word': index}
        # We sort by the index to ensure the order is correct for your model
        sorted_vocab = sorted(vocabulary.items(), key=lambda item: item[1])
        for word, index in sorted_vocab:
            f.write(word + '\n')

    # Add a check for the <unk> token, which is very important
    if '<unk>' not in vocabulary:
        print("Warning: Your vocabulary does not contain an '<unk>' token for unknown words.")


    # 2. Convert label_encoder.pkl -> labels.txt
    print("Loading label_encoder.pkl...")
    with open('label_encoder.pkl', 'rb') as f:
        label_encoder = pickle.load(f)

    print("Saving as labels.txt...")
    with open('labels.txt', 'w', encoding='utf-8') as f:
        # The labels are stored in the .classes_ attribute of the LabelEncoder
        for label in label_encoder.classes_:
            f.write(label + '\n')

    print("\nConversion successful!")
    print("You now have 'vocab.txt' and 'labels.txt' to use in your Android app.")

except FileNotFoundError as e:
    print(f"\nERROR: Could not find the file: {e.filename}")
    print("Please make sure 'vocabulary.pkl' and 'label_encoder.pkl' are in the same directory as this script.")
except Exception as e:
    print(f"\nAn unexpected error occurred: {e}")